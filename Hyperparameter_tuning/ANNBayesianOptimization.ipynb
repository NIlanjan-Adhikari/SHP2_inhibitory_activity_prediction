{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b602a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, auc, \\\n",
    "   precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dbf3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\Nilanjan\\Desktop\\paper\\Revision\\v1\\SHP2_inhibitory_activity_prediction\\10_fold_cross_validation\\train_10folds_208.csv\")\n",
    "data = data.iloc[:,1:]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0719201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TARGET'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f2ca0f-7940-458f-b75b-fc1de343e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([\"IC50(microM)\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db939887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    df = dataframe.iloc[:,:-1].copy()\n",
    "    labels = df.pop('TARGET')\n",
    "    df = {key: value[:,tf.newaxis] for key, value in dataframe.iloc[:,:-1].items()}\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(data))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c01d747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalization_layer(name, dataset):\n",
    "    # Create a Normalization layer for the feature.\n",
    "    normalizer = layers.Normalization(axis=None)\n",
    "    \n",
    "    # Prepare a Dataset that only yields the feature.\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    \n",
    "    # Learn the statistics of the data.\n",
    "    normalizer.adapt(feature_ds)\n",
    "    \n",
    "    return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b932b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "    \n",
    "    # create a layer that turns integer values into integer indices.\n",
    "    index = layers.IntegerLookup(max_tokens=max_tokens)\n",
    "    \n",
    "    # Prepare a `tf.data.Dataset` that only yields the feature.\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    \n",
    "    # Learn the set of possible values and assign them a fixed integer index.\n",
    "    index.adapt(feature_ds)\n",
    "    \n",
    "    # Encode the integer indices.\n",
    "    encoder = layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "    \n",
    "    # Apply multi-hot encoding to the indices. The lambda function captures the\n",
    "    # layer, so you can use them, or include them in the Keras Functional model later.\n",
    "    \n",
    "    return lambda feature: encoder(index(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cf1877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(data, fold, batch_size):\n",
    "    train = data[data.Kfold != fold].reset_index(drop=True)\n",
    "    valid = data[data.Kfold == fold].reset_index(drop=True)\n",
    "    train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "    val_ds = df_to_dataset(valid, shuffle=False, batch_size=batch_size)\n",
    "    \n",
    "    return train_ds, val_ds, train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9411f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(numerical_features, categorical_features):\n",
    "    all_inputs = []\n",
    "    encoded_features = []\n",
    "\n",
    "    # Numerical features.\n",
    "    for header in numerical_features:\n",
    "        numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "        normalization_layer = get_normalization_layer(header, train_ds)\n",
    "        encoded_numeric_col = normalization_layer(numeric_col)\n",
    "        all_inputs.append(numeric_col)\n",
    "        encoded_features.append(encoded_numeric_col)\n",
    "    \n",
    "    hacc_col = tf.keras.Input(shape=(1,), name='NumHAcceptors', dtype='int64')\n",
    "\n",
    "    encoding_layer = get_category_encoding_layer(name='NumHAcceptors',\n",
    "                                                 dataset=train_ds,\n",
    "                                                 dtype='int64',\n",
    "                                                 max_tokens=20)\n",
    "    encoded_hacc_col = encoding_layer(hacc_col)\n",
    "\n",
    "    all_inputs.append(hacc_col)\n",
    "    encoded_features.append(encoded_hacc_col)\n",
    "    \n",
    "    scc_col = tf.keras.Input(shape=(1,), name='NumSaturatedCarbocycles', dtype='int64')\n",
    "\n",
    "    encoding_layer = get_category_encoding_layer(name='NumSaturatedCarbocycles',\n",
    "                                                 dataset=train_ds,\n",
    "                                                 dtype='int64',\n",
    "                                                 max_tokens=5)\n",
    "    encoded_scc_col = encoding_layer(scc_col)\n",
    "\n",
    "    all_inputs.append(scc_col)\n",
    "    encoded_features.append(encoded_scc_col)\n",
    "    \n",
    "    fbc_col = tf.keras.Input(shape=(1,), name='fr_bicyclic', dtype='int64')\n",
    "\n",
    "    encoding_layer = get_category_encoding_layer(name='fr_bicyclic',\n",
    "                                                 dataset=train_ds,\n",
    "                                                 dtype='int64',\n",
    "                                                 max_tokens=10)\n",
    "    encoded_fbc_col = encoding_layer(fbc_col)\n",
    "    all_inputs.append(fbc_col)\n",
    "    encoded_features.append(encoded_fbc_col)\n",
    "    \n",
    "    return all_inputs, encoded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0213a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['MaxPartialCharge', 'FpDensityMorgan2', 'BCUT2D_CHGLO', 'BCUT2D_MRHI',\n",
    "       'PEOE_VSA12', 'PEOE_VSA6', 'SMR_VSA3', 'SlogP_VSA3', 'SlogP_VSA8', 'EState_VSA6']\n",
    "\n",
    "categorical_features = ['NumHAcceptors', 'NumSaturatedCarbocycles', 'fr_bicyclic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4472563",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    hidden_layer_1 = 16\n",
    "    hidden_layer_2 = 64\n",
    "    hidden_layer_3 = 128\n",
    "    hidden_layer_4 = 256\n",
    "    hidden_layer_5 = 32\n",
    "    hidden_layer_6 = 8\n",
    "    initializer = tf.keras.initializers.HeNormal()\n",
    "    lr = 0.001\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    dropout = 0.1\n",
    "    batch_size = 16\n",
    "    epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2d61c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AnnClassifier(all_inputs, encoded_features):\n",
    "    all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "    x = tf.keras.layers.Dense(units=config.hidden_layer_1, activation=\"relu\", \n",
    "                              kernel_initializer=config.initializer)(all_features)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    #x = tf.keras.layers.Dropout(hparams['dropout'][0])(x)\n",
    "    x = tf.keras.layers.Dense(units=config.hidden_layer_2, activation=\"relu\",\n",
    "                             kernel_initializer=config.initializer)(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    #x = tf.keras.layers.Dropout(hparams['dropout'][0])(x)\n",
    "    x = tf.keras.layers.Dense(units=config.hidden_layer_3, activation=\"relu\",\n",
    "                             kernel_initializer=config.initializer)(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    #x = tf.keras.layers.Dropout(hparams['dropout'][0])(x)\n",
    "    x = tf.keras.layers.Dense(units=config.hidden_layer_4, activation=\"relu\",\n",
    "                             kernel_initializer=config.initializer)(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    x = tf.keras.layers.Dense(units=config.hidden_layer_5, activation=\"relu\",\n",
    "                             kernel_initializer=config.initializer)(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    x = tf.keras.layers.Dense(units=config.hidden_layer_6, activation=\"relu\",\n",
    "                             kernel_initializer=config.initializer)(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(all_inputs, output)\n",
    "    model.compile(optimizer = config.optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba2754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(fold, all_inputs, encoded_features, valid, val_ds):\n",
    "    \n",
    "    model = AnnClassifier(all_inputs, encoded_features)\n",
    "    checkpoint = f\".\\\\model_checkpoints\\\\fold_{fold}\"\n",
    "    model.load_weights(checkpoint)\n",
    "    \n",
    "    y_pred_proba = model.predict(val_ds)\n",
    "    y_true = valid.TARGET.values\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    y_pred = (y_pred_proba > 0.5)\n",
    "    \n",
    "    accuracy = accuracy_score(y_true,y_pred)\n",
    "    precision_1 = precision_score(y_true,y_pred,pos_label=1)\n",
    "    precision_0 = precision_score(y_true,y_pred,pos_label=0)\n",
    "    recall_1 = recall_score(y_true,y_pred,pos_label=1)\n",
    "    recall_0 = recall_score(y_true,y_pred,pos_label=0)\n",
    "    f1score = f1_score(y_true,y_pred)\n",
    "    kappa = cohen_kappa_score(y_true,y_pred)\n",
    "    MCC = matthews_corrcoef(y_true,y_pred)\n",
    "    \n",
    "    print(f\"Fold = {fold}, AUC = {auc}, Accuracy = {accuracy}, \\\n",
    "          Precision_1 = {precision_1}, Precision_0 = {precision_0}\\\n",
    "          Recall_1 = {recall_1}, Recall_0 = {recall_0}, F1Score = {f1score}, kappa = {kappa}, MCC = {MCC}\")\n",
    "    \n",
    "    return auc, accuracy, precision_1, precision_0, recall_1, recall_0, f1score, kappa, MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914bae2a-a592-4e9e-8c82-9a3c3bb165f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = numerical_features + categorical_features + [\"TARGET\",\"Kfold\"]\n",
    "data = data[final_features]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecbca32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aucs, accuracies, precisions_1, precisions_0, recalls_1, recalls_0, f1scores, kappas, MCCs = [], [], [], [], [], [], [], [], []\n",
    "\n",
    "\n",
    "for fold_ in range(10):\n",
    "    train_ds, val_ds, train, valid = train_val_split(data, fold_, config.batch_size)\n",
    "    all_inputs, encoded_features = preprocess_dataset(numerical_features, categorical_features)\n",
    "    auc, accuracy, precision_1, precision_0, recall_1, recall_0, f1score, kappa, MCC = evaluate(fold_, all_inputs, encoded_features, valid, val_ds)\n",
    "    aucs.append(auc)\n",
    "    accuracies.append(accuracy)\n",
    "    precisions_1.append(precision_1)\n",
    "    precisions_0.append(precision_0)\n",
    "    recalls_1.append(recall_1)\n",
    "    recalls_0.append(recall_0)\n",
    "    f1scores.append(f1score)\n",
    "    kappas.append(kappa)\n",
    "    MCCs.append(MCC)\n",
    "    \n",
    "print(\"\\n\")\n",
    "print(f\"Mean Scores: AUC = {np.mean(np.array(aucs))}, \\\n",
    "      Accuracy = {np.mean(np.array(accuracies))}, \\\n",
    "      Precision_1 = {np.mean(np.array(precisions_1))}, Precision_0 = {np.mean(np.array(precisions_0))}\\\n",
    "      Recall_1 = {np.mean(np.array(recalls_1))}, Recall_0 = {np.mean(np.array(recalls_0))}\\\n",
    "      F1Score = {np.mean(np.array(f1scores))} \\\n",
    "      Kappa = {np.mean(np.array(kappas))} \\\n",
    "      MCC = {np.mean(np.array(MCCs))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceac251",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_metrics = pd.DataFrame(columns=['Accuracy','AUC','Precision_1','Precision_0','Recall_1','Recall_0','F1score','Kappa','MCC'])\n",
    "fold_metrics['Accuracy'] = np.array(accuracies)\n",
    "fold_metrics['AUC'] = np.array(aucs)\n",
    "fold_metrics['Precision_1'] = np.array(precisions_1)\n",
    "fold_metrics['Precision_0'] = np.array(precisions_0)\n",
    "fold_metrics['Recall_1'] = np.array(recalls_1)\n",
    "fold_metrics['Recall_0'] = np.array(recalls_0)\n",
    "fold_metrics['F1score'] = np.array(f1scores)\n",
    "fold_metrics['Kappa'] = np.array(kappas)\n",
    "fold_metrics['MCC'] = np.array(MCCs)\n",
    "fold_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ddce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_metrics.loc[10,:] = [np.mean(np.array(accuracies)), np.mean(np.array(aucs)), np.mean(np.array(precisions_1)),\n",
    "                               np.mean(np.array(precisions_0)), np.mean(np.array(recalls_1)), np.mean(np.array(recalls_0)),\n",
    "                            np.mean(np.array(f1scores)), np.mean(np.array(kappas)), np.mean(np.array(MCCs))]\n",
    "\n",
    "fold_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d285dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_metrics.loc[11,:] = [np.std(np.array(accuracies)), np.std(np.array(aucs)), np.std(np.array(precisions_1)),\n",
    "                               np.std(np.array(precisions_0)), np.std(np.array(recalls_1)), np.std(np.array(recalls_0)),\n",
    "                            np.std(np.array(f1scores)), np.std(np.array(kappas)), np.std(np.array(MCCs))]\n",
    "\n",
    "fold_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccd9895",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_metrics.index = ['Fold_0','Fold_1','Fold_2','Fold_3','Fold_4','Fold_5','Fold_6','Fold_7','Fold_8','Fold_9','Mean','Std']\n",
    "fold_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590851d4-e47a-4e15-9b58-fafbeb3f16a5",
   "metadata": {},
   "source": [
    "# Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91512626-7648-474d-9b55-3b03ad0c9464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2d1f55-0526-4168-afa8-2f1367c19d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AnnClassifier_(all_inputs, encoded_features,space):\n",
    "    all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "    x = tf.keras.layers.Dense(units=config.hidden_layer_1, activation=\"relu\", \n",
    "                              kernel_initializer=config.initializer)(all_features)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    #x = tf.keras.layers.Dropout(hparams['dropout'][0])(x)\n",
    "    x = tf.keras.layers.Dense(units=config.hidden_layer_2, activation=\"relu\",\n",
    "                             kernel_initializer=config.initializer)(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    #x = tf.keras.layers.Dropout(hparams['dropout'][0])(x)\n",
    "    x = tf.keras.layers.Dense(units=config.hidden_layer_3, activation=\"relu\",\n",
    "                             kernel_initializer=config.initializer)(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    #x = tf.keras.layers.Dropout(hparams['dropout'][0])(x)\n",
    "    x = tf.keras.layers.Dense(units=config.hidden_layer_4, activation=\"relu\",\n",
    "                             kernel_initializer=config.initializer)(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    x = tf.keras.layers.Dense(units=config.hidden_layer_5, activation=\"relu\",\n",
    "                             kernel_initializer=config.initializer)(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    x = tf.keras.layers.Dense(units=config.hidden_layer_6, activation=\"relu\",\n",
    "                             kernel_initializer=config.initializer)(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(all_inputs, output)\n",
    "    config.lr = space[\"learning_rate\"]\n",
    "    model.compile(optimizer = config.optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4375c1fd-134c-4cbc-b0f5-720488f98dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianOPT:\n",
    "    def __init__(self, data, space):\n",
    "        self.space = space\n",
    "        self.data = data\n",
    "        self.trials = Trials()\n",
    "    \n",
    "    def _objective(self,space):\n",
    "        model = AnnClassifier_(all_inputs, encoded_features, space)\n",
    "        accuracies = []\n",
    "        for fold_ in range(10):\n",
    "            train_ds, val_ds, train, valid = train_val_split(data, fold_, config.batch_size)\n",
    "            \n",
    "            # evaluation = [( X_train, y_train), ( X_test, y_test)]\n",
    "        \n",
    "\n",
    "            checkpoint = f\".\\\\model_checkpoints\\\\fold_{fold_}\"\n",
    "            model.load_weights(checkpoint)\n",
    "\n",
    "\n",
    "            pred = model.predict(val_ds)\n",
    "            y_test = valid.TARGET.values\n",
    "            accuracy = accuracy_score(y_test, pred>0.5)\n",
    "            accuracies.append(accuracy)\n",
    "        \n",
    "        final_accuracy = np.mean(np.array(accuracies))\n",
    "        #print (\"SCORE:\", final_accuracy)\n",
    "        return {'loss': -final_accuracy, 'status': STATUS_OK }\n",
    "    \n",
    "    def search_hyperparameters(self):\n",
    "\n",
    "        best_hyperparams = fmin(fn = self._objective,\n",
    "                                space = self.space,\n",
    "                                algo = tpe.suggest,\n",
    "                                max_evals = 500,\n",
    "                                trials = self.trials)\n",
    "        return best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f72728-461c-4a4f-9916-eb0e46d043de",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.0001), np.log(0.1)),\n",
    "    'epochs': hp.choice('epochs', [10, 20, 30, 40, 50]),\n",
    "    'batch_size': hp.choice('batch_size', [16, 32, 64, 128]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1b0093-8e36-4a8e-b137-46c0c3ff5eb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bayes_opt = BayesianOPT(data, space)\n",
    "best_params = bayes_opt.search_hyperparameters()\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a158be-76ca-4f97-8471-f871c2747033",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params[\"epochs\"] = 10\n",
    "best_params[\"batch_size\"] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5cf3e0-626b-4f86-8a77-a67c89634c87",
   "metadata": {},
   "source": [
    "# Train the model with optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d29397c-efb8-4678-89b4-8049515a57e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fold, all_inputs, encoded_features, train_ds, val_ds, epochs):\n",
    "    \n",
    "    print(f\"Fold : {fold}\")\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "    \n",
    "    logdir = f'.\\\\tensorboard_logs\\\\scalars\\\\fold_{fold}\\\\'\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir,write_graph=True,update_freq=1)\n",
    "    \n",
    "    checkpoint_filepath = f'.\\\\model_checkpoints\\\\fold_{fold}\\\\'\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True)\n",
    "    \n",
    "    model = AnnClassifier_(all_inputs, encoded_features, best_params)\n",
    "\n",
    "    model.fit(train_ds, \n",
    "            epochs = epochs, \n",
    "            validation_data = val_ds,\n",
    "            callbacks=[early_stop, tensorboard_callback, model_checkpoint_callback])\n",
    "    \n",
    "    # model_loss = pd.DataFrame(log.history.history)\n",
    "    # model_loss.to_csv(f\".\\\\Metrics_\\\\metrics_{fold}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b7effd-0907-43d7-a68c-327dfa98a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_lambda(x):\n",
    "    return tf.cast(x, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb3af5-5d80-435a-8e9a-0e019ae0ecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold, data, best_params):\n",
    "    train_ds, val_ds, train_, valid = train_val_split(data, fold_, best_params[\"batch_size\"])\n",
    "    # initialize Logistic Regression model\n",
    "    # train_ds = train_ds.map(my_lambda)\n",
    "    # val_ds = val_ds.map(my_lambda)\n",
    "    all_inputs, encoded_features = preprocess_dataset(numerical_features, categorical_features)\n",
    "    train(fold, all_inputs, encoded_features, train_ds, val_ds, best_params[\"epochs\"])\n",
    "    model = AnnClassifier_(all_inputs, encoded_features, best_params)\n",
    "    checkpoint = f\".\\\\model_checkpoints\\\\fold_{fold}\"\n",
    "    model.load_weights(checkpoint)\n",
    "    y_pred = model.predict(val_ds)\n",
    "    y_pred = y_pred > 0.5\n",
    "    y_true = valid.TARGET.values\n",
    "    accuracy = accuracy_score(y_true,y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    precision_1 = precision_score(y_true,y_pred,pos_label=1)\n",
    "    precision_0 = precision_score(y_true,y_pred,pos_label=0)\n",
    "    recall_1 = recall_score(y_true,y_pred,pos_label=1)\n",
    "    recall_0 = recall_score(y_true,y_pred,pos_label=0)\n",
    "    f1score = f1_score(y_true,y_pred)\n",
    "    kappa = cohen_kappa_score(y_true,y_pred)\n",
    "    MCC = matthews_corrcoef(y_true,y_pred)\n",
    "    print(f\"Fold = {fold}, AUC = {auc}, Accuracy = {accuracy}, \\\n",
    "          Precision_1 = {precision_1}, Precision_0 = {precision_0}\\\n",
    "          Recall_1 = {recall_1}, Recall_0 = {recall_0}, F1Score = {f1score}, kappa = {kappa}, MCC = {MCC}\")\n",
    "    \n",
    "    return auc, accuracy, precision_1, precision_0, recall_1, recall_0, f1score, kappa, MCC, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7cb453-7d93-4bea-ae3a-36f983805c4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aucs, accuracies, precisions_1, precisions_0, recalls_1, recalls_0, f1scores, kappas, MCCs = [], [], [], [], [], [], [], [], []\n",
    "\n",
    "for fold_ in range(10):\n",
    "    auc, accuracy, precision_1, precision_0, recall_1, recall_0, f1score, kappa, MCC, model = run(fold_, data, best_params)\n",
    "    aucs.append(auc)\n",
    "    accuracies.append(accuracy)\n",
    "    precisions_1.append(precision_1)\n",
    "    precisions_0.append(precision_0)\n",
    "    recalls_1.append(recall_1)\n",
    "    recalls_0.append(recall_0)\n",
    "    f1scores.append(f1score)\n",
    "    kappas.append(kappa)\n",
    "    MCCs.append(MCC)\n",
    "    \n",
    "print(\"\\n\")\n",
    "print(f\"Mean Scores: AUC = {np.mean(np.array(aucs))}, \\\n",
    "      Accuracy = {np.mean(np.array(accuracies))}, \\\n",
    "      Precision_1 = {np.mean(np.array(precisions_1))}, Precision_0 = {np.mean(np.array(precisions_0))}\\\n",
    "      Recall_1 = {np.mean(np.array(recalls_1))}, Recall_0 = {np.mean(np.array(recalls_0))}\\\n",
    "      F1Score = {np.mean(np.array(f1scores))} \\\n",
    "      Kappa = {np.mean(np.array(kappas))} \\\n",
    "      MCC = {np.mean(np.array(MCCs))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deed70a1-c130-4e9b-a00b-2d2dd5827d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
