{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40156c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, auc, \\\n",
    "  ConfusionMatrixDisplay, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ce82414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MaxPartialCharge</th>\n",
       "      <th>FpDensityMorgan2</th>\n",
       "      <th>BCUT2D_CHGLO</th>\n",
       "      <th>BCUT2D_MRHI</th>\n",
       "      <th>PEOE_VSA12</th>\n",
       "      <th>PEOE_VSA6</th>\n",
       "      <th>SMR_VSA3</th>\n",
       "      <th>SlogP_VSA3</th>\n",
       "      <th>SlogP_VSA8</th>\n",
       "      <th>EState_VSA6</th>\n",
       "      <th>NumHAcceptors</th>\n",
       "      <th>NumSaturatedCarbocycles</th>\n",
       "      <th>fr_bicyclic</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>Kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.335201</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>-2.072068</td>\n",
       "      <td>5.975550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.132734</td>\n",
       "      <td>9.551078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.974594</td>\n",
       "      <td>43.638476</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.226791</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-2.362642</td>\n",
       "      <td>7.150190</td>\n",
       "      <td>5.948339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.967957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.440599</td>\n",
       "      <td>11.336786</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.211302</td>\n",
       "      <td>1.772727</td>\n",
       "      <td>-2.089721</td>\n",
       "      <td>7.912349</td>\n",
       "      <td>9.837253</td>\n",
       "      <td>29.297126</td>\n",
       "      <td>4.983979</td>\n",
       "      <td>9.837253</td>\n",
       "      <td>10.902925</td>\n",
       "      <td>24.265468</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.226898</td>\n",
       "      <td>1.869565</td>\n",
       "      <td>-2.357502</td>\n",
       "      <td>6.433493</td>\n",
       "      <td>5.948339</td>\n",
       "      <td>35.334614</td>\n",
       "      <td>15.284746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.126903</td>\n",
       "      <td>12.263211</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.158370</td>\n",
       "      <td>1.870968</td>\n",
       "      <td>-2.421374</td>\n",
       "      <td>7.991366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.362825</td>\n",
       "      <td>19.935914</td>\n",
       "      <td>6.420822</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.460054</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2734</th>\n",
       "      <td>0.152613</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-2.415835</td>\n",
       "      <td>6.432407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.334614</td>\n",
       "      <td>9.967957</td>\n",
       "      <td>11.343745</td>\n",
       "      <td>11.257379</td>\n",
       "      <td>6.066367</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2735</th>\n",
       "      <td>0.335203</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>-2.001320</td>\n",
       "      <td>6.164147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.331835</td>\n",
       "      <td>4.983979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.902925</td>\n",
       "      <td>36.528679</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2736</th>\n",
       "      <td>0.264178</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>-2.414068</td>\n",
       "      <td>5.737009</td>\n",
       "      <td>5.948339</td>\n",
       "      <td>38.112943</td>\n",
       "      <td>9.551078</td>\n",
       "      <td>11.784535</td>\n",
       "      <td>11.126903</td>\n",
       "      <td>11.614772</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2737</th>\n",
       "      <td>0.338995</td>\n",
       "      <td>1.815789</td>\n",
       "      <td>-2.065301</td>\n",
       "      <td>7.218862</td>\n",
       "      <td>5.907180</td>\n",
       "      <td>59.014740</td>\n",
       "      <td>24.544948</td>\n",
       "      <td>18.386966</td>\n",
       "      <td>43.634305</td>\n",
       "      <td>6.196844</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2738</th>\n",
       "      <td>0.282757</td>\n",
       "      <td>1.789474</td>\n",
       "      <td>-2.330174</td>\n",
       "      <td>9.102997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.147817</td>\n",
       "      <td>9.799819</td>\n",
       "      <td>4.794537</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2739 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MaxPartialCharge  FpDensityMorgan2  BCUT2D_CHGLO  BCUT2D_MRHI  \\\n",
       "0             0.335201          1.714286     -2.072068     5.975550   \n",
       "1             0.226791          2.000000     -2.362642     7.150190   \n",
       "2             0.211302          1.772727     -2.089721     7.912349   \n",
       "3             0.226898          1.869565     -2.357502     6.433493   \n",
       "4             0.158370          1.870968     -2.421374     7.991366   \n",
       "...                ...               ...           ...          ...   \n",
       "2734          0.152613          2.000000     -2.415835     6.432407   \n",
       "2735          0.335203          1.700000     -2.001320     6.164147   \n",
       "2736          0.264178          1.833333     -2.414068     5.737009   \n",
       "2737          0.338995          1.815789     -2.065301     7.218862   \n",
       "2738          0.282757          1.789474     -2.330174     9.102997   \n",
       "\n",
       "      PEOE_VSA12  PEOE_VSA6   SMR_VSA3  SlogP_VSA3  SlogP_VSA8  EState_VSA6  \\\n",
       "0       0.000000  12.132734   9.551078    0.000000   38.974594    43.638476   \n",
       "1       5.948339   0.000000   9.967957    0.000000   10.440599    11.336786   \n",
       "2       9.837253  29.297126   4.983979    9.837253   10.902925    24.265468   \n",
       "3       5.948339  35.334614  15.284746    0.000000   11.126903    12.263211   \n",
       "4       0.000000  23.362825  19.935914    6.420822    0.000000    18.460054   \n",
       "...          ...        ...        ...         ...         ...          ...   \n",
       "2734    0.000000  35.334614   9.967957   11.343745   11.257379     6.066367   \n",
       "2735    0.000000  30.331835   4.983979    0.000000   10.902925    36.528679   \n",
       "2736    5.948339  38.112943   9.551078   11.784535   11.126903    11.614772   \n",
       "2737    5.907180  59.014740  24.544948   18.386966   43.634305     6.196844   \n",
       "2738    0.000000  28.147817   9.799819    4.794537    0.000000     0.000000   \n",
       "\n",
       "      NumHAcceptors  NumSaturatedCarbocycles  fr_bicyclic  TARGET  Kfold  \n",
       "0                 6                        0            1     1.0      0  \n",
       "1                 6                        0            0     1.0      1  \n",
       "2                 4                        0            1     0.0      1  \n",
       "3                 5                        0            0     0.0      0  \n",
       "4                 9                        0            1     1.0      4  \n",
       "...             ...                      ...          ...     ...    ...  \n",
       "2734              6                        0            0     1.0      3  \n",
       "2735              2                        0            1     0.0      6  \n",
       "2736              7                        0            0     1.0      8  \n",
       "2737              9                        0            2     0.0      5  \n",
       "2738              3                        0            0     1.0      5  \n",
       "\n",
       "[2739 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r'..\\..\\10_fold_cross_validation\\train_10folds.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57b6fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold, data):\n",
    "    # load the full training data with folds\n",
    "    df = data\n",
    "    # all columns are features except target and kfold columns\n",
    "    features = [\n",
    "        f for f in df.columns if f not in (\"TARGET\", \"Kfold\")\n",
    "    ]\n",
    "    # get training data using folds\n",
    "    df_train = df[df.Kfold != fold].reset_index(drop=True)\n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.Kfold == fold].reset_index(drop=True)\n",
    "    # get training data\n",
    "    X_train = df_train[features].values\n",
    "    # get validation data\n",
    "    X_valid = df_valid[features].values\n",
    "    # initialize Logistic Regression model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, df_train.TARGET.values)\n",
    "    valid_preds = model.predict_proba(X_valid)[:, 1]\n",
    "    auc = roc_auc_score(df_valid.TARGET.values, valid_preds)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    y_true = df_valid.TARGET.values\n",
    "    accuracy = accuracy_score(y_true,y_pred)\n",
    "    precision_1 = precision_score(y_true,y_pred,pos_label=1)\n",
    "    precision_0 = precision_score(y_true,y_pred,pos_label=0)\n",
    "    recall_1 = recall_score(y_true,y_pred,pos_label=1)\n",
    "    recall_0 = recall_score(y_true,y_pred,pos_label=0)\n",
    "    f1score = f1_score(y_true,y_pred)\n",
    "    kappa = cohen_kappa_score(y_true,y_pred)\n",
    "    MCC = matthews_corrcoef(y_true,y_pred)\n",
    "    print(f\"Fold = {fold}, AUC = {auc}, Accuracy = {accuracy}, \\\n",
    "          Precision_1 = {precision_1}, Precision_0 = {precision_0}\\\n",
    "          Recall_1 = {recall_1}, Recall_0 = {recall_0}, F1Score = {f1score}, kappa = {kappa}, MCC = {MCC}\")\n",
    "    \n",
    "    return auc, accuracy, precision_1, precision_0, recall_1, recall_0, f1score, kappa, MCC, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "723cb578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\MY_FILES\\Anaconda\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "E:\\MY_FILES\\Anaconda\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "E:\\MY_FILES\\Anaconda\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 0, AUC = 0.8591065292096219, Accuracy = 0.7992700729927007,           Precision_1 = 0.723404255319149, Precision_0 = 0.8388888888888889          Recall_1 = 0.7010309278350515, Recall_0 = 0.8531073446327684, F1Score = 0.7120418848167538, kappa = 0.5580385946389818, MCC = 0.5582008165095612\n",
      "Fold = 1, AUC = 0.8756479701788106, Accuracy = 0.8211678832116789,           Precision_1 = 0.7926829268292683, Precision_0 = 0.8333333333333334          Recall_1 = 0.6701030927835051, Recall_0 = 0.903954802259887, F1Score = 0.7262569832402234, kappa = 0.594845796366709, MCC = 0.5994744169452769\n",
      "Fold = 2, AUC = 0.8626594443473702, Accuracy = 0.7846715328467153,           Precision_1 = 0.7065217391304348, Precision_0 = 0.8241758241758241          Recall_1 = 0.6701030927835051, Recall_0 = 0.847457627118644, F1Score = 0.6878306878306877, kappa = 0.5236607932111497, MCC = 0.5240879820365124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\MY_FILES\\Anaconda\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "E:\\MY_FILES\\Anaconda\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "E:\\MY_FILES\\Anaconda\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 3, AUC = 0.8747743025219873, Accuracy = 0.8211678832116789,           Precision_1 = 0.8, Precision_0 = 0.8298969072164949          Recall_1 = 0.6597938144329897, Recall_0 = 0.9096045197740112, F1Score = 0.7231638418079096, kappa = 0.592880101886106, MCC = 0.5988841705131419\n",
      "Fold = 4, AUC = 0.8800745529733822, Accuracy = 0.8248175182481752,           Precision_1 = 0.7951807228915663, Precision_0 = 0.837696335078534          Recall_1 = 0.6804123711340206, Recall_0 = 0.903954802259887, F1Score = 0.7333333333333334, kappa = 0.6040700824853995, MCC = 0.6081386169878046\n",
      "Fold = 5, AUC = 0.9068670277826314, Accuracy = 0.8394160583941606,           Precision_1 = 0.7912087912087912, Precision_0 = 0.8633879781420765          Recall_1 = 0.7422680412371134, Recall_0 = 0.8926553672316384, F1Score = 0.7659574468085107, kappa = 0.6439246263807668, MCC = 0.644685048662435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\MY_FILES\\Anaconda\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "E:\\MY_FILES\\Anaconda\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "E:\\MY_FILES\\Anaconda\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 6, AUC = 0.8603879084396295, Accuracy = 0.7883211678832117,           Precision_1 = 0.7294117647058823, Precision_0 = 0.8148148148148148          Recall_1 = 0.6391752577319587, Recall_0 = 0.8700564971751412, F1Score = 0.6813186813186812, kappa = 0.5238780034753431, MCC = 0.5264384637888962\n",
      "Fold = 7, AUC = 0.8469043042693226, Accuracy = 0.7846715328467153,           Precision_1 = 0.6862745098039216, Precision_0 = 0.8430232558139535          Recall_1 = 0.7216494845360825, Recall_0 = 0.8192090395480226, F1Score = 0.7035175879396985, kappa = 0.5346306638263573, MCC = 0.5350469215994972\n",
      "Fold = 8, AUC = 0.9093515917602997, Accuracy = 0.8357664233576643,           Precision_1 = 0.7524752475247525, Precision_0 = 0.884393063583815          Recall_1 = 0.7916666666666666, Recall_0 = 0.8595505617977528, F1Score = 0.7715736040609136, kappa = 0.6434973688775806, MCC = 0.64400280780206\n",
      "Fold = 9, AUC = 0.8612876647834276, Accuracy = 0.8021978021978022,           Precision_1 = 0.71, Precision_0 = 0.8554913294797688          Recall_1 = 0.7395833333333334, Recall_0 = 0.8361581920903954, F1Score = 0.7244897959183674, kappa = 0.5703043022035676, MCC = 0.5705934109754287\n",
      "\n",
      "\n",
      "Mean Scores: AUC = 0.8737061296266484,       Accuracy = 0.8101467875190502,       Precision_1 = 0.7487159957413766, Precision_0 = 0.8425101730527503      Recall_1 = 0.7015786082474227, Recall_0 = 0.8695708753888148      F1Score = 0.7229483847075079       Kappa = 0.5789730333351961       MCC = 0.5809552655820613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\MY_FILES\\Anaconda\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "aucs, accuracies, precisions_1, precisions_0, recalls_1, recalls_0, f1scores, kappas, MCCs = [], [], [], [], [], [], [], [], []\n",
    "\n",
    "for fold_ in range(10):\n",
    "    auc, accuracy, precision_1, precision_0, recall_1, recall_0, f1score, kappa, MCC, model = run(fold_, data)\n",
    "    aucs.append(auc)\n",
    "    accuracies.append(accuracy)\n",
    "    precisions_1.append(precision_1)\n",
    "    precisions_0.append(precision_0)\n",
    "    recalls_1.append(recall_1)\n",
    "    recalls_0.append(recall_0)\n",
    "    f1scores.append(f1score)\n",
    "    kappas.append(kappa)\n",
    "    MCCs.append(MCC)\n",
    "    filename = 'LR_' + str(fold_) + '.pkl'\n",
    "    joblib.dump(model, filename)\n",
    "print(\"\\n\")\n",
    "print(f\"Mean Scores: AUC = {np.mean(np.array(aucs))}, \\\n",
    "      Accuracy = {np.mean(np.array(accuracies))}, \\\n",
    "      Precision_1 = {np.mean(np.array(precisions_1))}, Precision_0 = {np.mean(np.array(precisions_0))}\\\n",
    "      Recall_1 = {np.mean(np.array(recalls_1))}, Recall_0 = {np.mean(np.array(recalls_0))}\\\n",
    "      F1Score = {np.mean(np.array(f1scores))} \\\n",
    "      Kappa = {np.mean(np.array(kappas))} \\\n",
    "      MCC = {np.mean(np.array(MCCs))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d66d9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Precision_1</th>\n",
       "      <th>Precision_0</th>\n",
       "      <th>Recall_1</th>\n",
       "      <th>Recall_0</th>\n",
       "      <th>F1score</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.799270</td>\n",
       "      <td>0.859107</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>0.838889</td>\n",
       "      <td>0.701031</td>\n",
       "      <td>0.853107</td>\n",
       "      <td>0.712042</td>\n",
       "      <td>0.558039</td>\n",
       "      <td>0.558201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.821168</td>\n",
       "      <td>0.875648</td>\n",
       "      <td>0.792683</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.670103</td>\n",
       "      <td>0.903955</td>\n",
       "      <td>0.726257</td>\n",
       "      <td>0.594846</td>\n",
       "      <td>0.599474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.784672</td>\n",
       "      <td>0.862659</td>\n",
       "      <td>0.706522</td>\n",
       "      <td>0.824176</td>\n",
       "      <td>0.670103</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.687831</td>\n",
       "      <td>0.523661</td>\n",
       "      <td>0.524088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.821168</td>\n",
       "      <td>0.874774</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.829897</td>\n",
       "      <td>0.659794</td>\n",
       "      <td>0.909605</td>\n",
       "      <td>0.723164</td>\n",
       "      <td>0.592880</td>\n",
       "      <td>0.598884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.824818</td>\n",
       "      <td>0.880075</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.837696</td>\n",
       "      <td>0.680412</td>\n",
       "      <td>0.903955</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.604070</td>\n",
       "      <td>0.608139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.839416</td>\n",
       "      <td>0.906867</td>\n",
       "      <td>0.791209</td>\n",
       "      <td>0.863388</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>0.892655</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.643925</td>\n",
       "      <td>0.644685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.788321</td>\n",
       "      <td>0.860388</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.639175</td>\n",
       "      <td>0.870056</td>\n",
       "      <td>0.681319</td>\n",
       "      <td>0.523878</td>\n",
       "      <td>0.526438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.784672</td>\n",
       "      <td>0.846904</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.843023</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>0.819209</td>\n",
       "      <td>0.703518</td>\n",
       "      <td>0.534631</td>\n",
       "      <td>0.535047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.835766</td>\n",
       "      <td>0.909352</td>\n",
       "      <td>0.752475</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.859551</td>\n",
       "      <td>0.771574</td>\n",
       "      <td>0.643497</td>\n",
       "      <td>0.644003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.802198</td>\n",
       "      <td>0.861288</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.855491</td>\n",
       "      <td>0.739583</td>\n",
       "      <td>0.836158</td>\n",
       "      <td>0.724490</td>\n",
       "      <td>0.570304</td>\n",
       "      <td>0.570593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy       AUC  Precision_1  Precision_0  Recall_1  Recall_0   F1score  \\\n",
       "0  0.799270  0.859107     0.723404     0.838889  0.701031  0.853107  0.712042   \n",
       "1  0.821168  0.875648     0.792683     0.833333  0.670103  0.903955  0.726257   \n",
       "2  0.784672  0.862659     0.706522     0.824176  0.670103  0.847458  0.687831   \n",
       "3  0.821168  0.874774     0.800000     0.829897  0.659794  0.909605  0.723164   \n",
       "4  0.824818  0.880075     0.795181     0.837696  0.680412  0.903955  0.733333   \n",
       "5  0.839416  0.906867     0.791209     0.863388  0.742268  0.892655  0.765957   \n",
       "6  0.788321  0.860388     0.729412     0.814815  0.639175  0.870056  0.681319   \n",
       "7  0.784672  0.846904     0.686275     0.843023  0.721649  0.819209  0.703518   \n",
       "8  0.835766  0.909352     0.752475     0.884393  0.791667  0.859551  0.771574   \n",
       "9  0.802198  0.861288     0.710000     0.855491  0.739583  0.836158  0.724490   \n",
       "\n",
       "      Kappa       MCC  \n",
       "0  0.558039  0.558201  \n",
       "1  0.594846  0.599474  \n",
       "2  0.523661  0.524088  \n",
       "3  0.592880  0.598884  \n",
       "4  0.604070  0.608139  \n",
       "5  0.643925  0.644685  \n",
       "6  0.523878  0.526438  \n",
       "7  0.534631  0.535047  \n",
       "8  0.643497  0.644003  \n",
       "9  0.570304  0.570593  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_metrics = pd.DataFrame(columns=['Accuracy','AUC','Precision_1','Precision_0','Recall_1','Recall_0','F1score','Kappa','MCC'])\n",
    "fold_metrics['Accuracy'] = np.array(accuracies)\n",
    "fold_metrics['AUC'] = np.array(aucs)\n",
    "fold_metrics['Precision_1'] = np.array(precisions_1)\n",
    "fold_metrics['Precision_0'] = np.array(precisions_0)\n",
    "fold_metrics['Recall_1'] = np.array(recalls_1)\n",
    "fold_metrics['Recall_0'] = np.array(recalls_0)\n",
    "fold_metrics['F1score'] = np.array(f1scores)\n",
    "fold_metrics['Kappa'] = np.array(kappas)\n",
    "fold_metrics['MCC'] = np.array(MCCs)\n",
    "fold_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76b997b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Precision_1</th>\n",
       "      <th>Precision_0</th>\n",
       "      <th>Recall_1</th>\n",
       "      <th>Recall_0</th>\n",
       "      <th>F1score</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.799270</td>\n",
       "      <td>0.859107</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>0.838889</td>\n",
       "      <td>0.701031</td>\n",
       "      <td>0.853107</td>\n",
       "      <td>0.712042</td>\n",
       "      <td>0.558039</td>\n",
       "      <td>0.558201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.821168</td>\n",
       "      <td>0.875648</td>\n",
       "      <td>0.792683</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.670103</td>\n",
       "      <td>0.903955</td>\n",
       "      <td>0.726257</td>\n",
       "      <td>0.594846</td>\n",
       "      <td>0.599474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.784672</td>\n",
       "      <td>0.862659</td>\n",
       "      <td>0.706522</td>\n",
       "      <td>0.824176</td>\n",
       "      <td>0.670103</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.687831</td>\n",
       "      <td>0.523661</td>\n",
       "      <td>0.524088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.821168</td>\n",
       "      <td>0.874774</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.829897</td>\n",
       "      <td>0.659794</td>\n",
       "      <td>0.909605</td>\n",
       "      <td>0.723164</td>\n",
       "      <td>0.592880</td>\n",
       "      <td>0.598884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.824818</td>\n",
       "      <td>0.880075</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.837696</td>\n",
       "      <td>0.680412</td>\n",
       "      <td>0.903955</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.604070</td>\n",
       "      <td>0.608139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.839416</td>\n",
       "      <td>0.906867</td>\n",
       "      <td>0.791209</td>\n",
       "      <td>0.863388</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>0.892655</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.643925</td>\n",
       "      <td>0.644685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.788321</td>\n",
       "      <td>0.860388</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.639175</td>\n",
       "      <td>0.870056</td>\n",
       "      <td>0.681319</td>\n",
       "      <td>0.523878</td>\n",
       "      <td>0.526438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.784672</td>\n",
       "      <td>0.846904</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.843023</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>0.819209</td>\n",
       "      <td>0.703518</td>\n",
       "      <td>0.534631</td>\n",
       "      <td>0.535047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.835766</td>\n",
       "      <td>0.909352</td>\n",
       "      <td>0.752475</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.859551</td>\n",
       "      <td>0.771574</td>\n",
       "      <td>0.643497</td>\n",
       "      <td>0.644003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.802198</td>\n",
       "      <td>0.861288</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.855491</td>\n",
       "      <td>0.739583</td>\n",
       "      <td>0.836158</td>\n",
       "      <td>0.724490</td>\n",
       "      <td>0.570304</td>\n",
       "      <td>0.570593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.810147</td>\n",
       "      <td>0.873706</td>\n",
       "      <td>0.748716</td>\n",
       "      <td>0.842510</td>\n",
       "      <td>0.701579</td>\n",
       "      <td>0.869571</td>\n",
       "      <td>0.722948</td>\n",
       "      <td>0.578973</td>\n",
       "      <td>0.580955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Accuracy       AUC  Precision_1  Precision_0  Recall_1  Recall_0  \\\n",
       "0   0.799270  0.859107     0.723404     0.838889  0.701031  0.853107   \n",
       "1   0.821168  0.875648     0.792683     0.833333  0.670103  0.903955   \n",
       "2   0.784672  0.862659     0.706522     0.824176  0.670103  0.847458   \n",
       "3   0.821168  0.874774     0.800000     0.829897  0.659794  0.909605   \n",
       "4   0.824818  0.880075     0.795181     0.837696  0.680412  0.903955   \n",
       "5   0.839416  0.906867     0.791209     0.863388  0.742268  0.892655   \n",
       "6   0.788321  0.860388     0.729412     0.814815  0.639175  0.870056   \n",
       "7   0.784672  0.846904     0.686275     0.843023  0.721649  0.819209   \n",
       "8   0.835766  0.909352     0.752475     0.884393  0.791667  0.859551   \n",
       "9   0.802198  0.861288     0.710000     0.855491  0.739583  0.836158   \n",
       "10  0.810147  0.873706     0.748716     0.842510  0.701579  0.869571   \n",
       "\n",
       "     F1score     Kappa       MCC  \n",
       "0   0.712042  0.558039  0.558201  \n",
       "1   0.726257  0.594846  0.599474  \n",
       "2   0.687831  0.523661  0.524088  \n",
       "3   0.723164  0.592880  0.598884  \n",
       "4   0.733333  0.604070  0.608139  \n",
       "5   0.765957  0.643925  0.644685  \n",
       "6   0.681319  0.523878  0.526438  \n",
       "7   0.703518  0.534631  0.535047  \n",
       "8   0.771574  0.643497  0.644003  \n",
       "9   0.724490  0.570304  0.570593  \n",
       "10  0.722948  0.578973  0.580955  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_metrics.loc[10,:] = [np.mean(np.array(accuracies)), np.mean(np.array(aucs)), np.mean(np.array(precisions_1)),\n",
    "                               np.mean(np.array(precisions_0)), np.mean(np.array(recalls_1)), np.mean(np.array(recalls_0)),\n",
    "                            np.mean(np.array(f1scores)), np.mean(np.array(kappas)), np.mean(np.array(MCCs))]\n",
    "\n",
    "fold_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a729d867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Precision_1</th>\n",
       "      <th>Precision_0</th>\n",
       "      <th>Recall_1</th>\n",
       "      <th>Recall_0</th>\n",
       "      <th>F1score</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.799270</td>\n",
       "      <td>0.859107</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>0.838889</td>\n",
       "      <td>0.701031</td>\n",
       "      <td>0.853107</td>\n",
       "      <td>0.712042</td>\n",
       "      <td>0.558039</td>\n",
       "      <td>0.558201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.821168</td>\n",
       "      <td>0.875648</td>\n",
       "      <td>0.792683</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.670103</td>\n",
       "      <td>0.903955</td>\n",
       "      <td>0.726257</td>\n",
       "      <td>0.594846</td>\n",
       "      <td>0.599474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.784672</td>\n",
       "      <td>0.862659</td>\n",
       "      <td>0.706522</td>\n",
       "      <td>0.824176</td>\n",
       "      <td>0.670103</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.687831</td>\n",
       "      <td>0.523661</td>\n",
       "      <td>0.524088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.821168</td>\n",
       "      <td>0.874774</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.829897</td>\n",
       "      <td>0.659794</td>\n",
       "      <td>0.909605</td>\n",
       "      <td>0.723164</td>\n",
       "      <td>0.592880</td>\n",
       "      <td>0.598884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.824818</td>\n",
       "      <td>0.880075</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.837696</td>\n",
       "      <td>0.680412</td>\n",
       "      <td>0.903955</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.604070</td>\n",
       "      <td>0.608139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.839416</td>\n",
       "      <td>0.906867</td>\n",
       "      <td>0.791209</td>\n",
       "      <td>0.863388</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>0.892655</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.643925</td>\n",
       "      <td>0.644685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.788321</td>\n",
       "      <td>0.860388</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.639175</td>\n",
       "      <td>0.870056</td>\n",
       "      <td>0.681319</td>\n",
       "      <td>0.523878</td>\n",
       "      <td>0.526438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.784672</td>\n",
       "      <td>0.846904</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.843023</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>0.819209</td>\n",
       "      <td>0.703518</td>\n",
       "      <td>0.534631</td>\n",
       "      <td>0.535047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.835766</td>\n",
       "      <td>0.909352</td>\n",
       "      <td>0.752475</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.859551</td>\n",
       "      <td>0.771574</td>\n",
       "      <td>0.643497</td>\n",
       "      <td>0.644003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.802198</td>\n",
       "      <td>0.861288</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.855491</td>\n",
       "      <td>0.739583</td>\n",
       "      <td>0.836158</td>\n",
       "      <td>0.724490</td>\n",
       "      <td>0.570304</td>\n",
       "      <td>0.570593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.810147</td>\n",
       "      <td>0.873706</td>\n",
       "      <td>0.748716</td>\n",
       "      <td>0.842510</td>\n",
       "      <td>0.701579</td>\n",
       "      <td>0.869571</td>\n",
       "      <td>0.722948</td>\n",
       "      <td>0.578973</td>\n",
       "      <td>0.580955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.019817</td>\n",
       "      <td>0.019496</td>\n",
       "      <td>0.040907</td>\n",
       "      <td>0.019343</td>\n",
       "      <td>0.044372</td>\n",
       "      <td>0.030022</td>\n",
       "      <td>0.027925</td>\n",
       "      <td>0.042464</td>\n",
       "      <td>0.042870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Accuracy       AUC  Precision_1  Precision_0  Recall_1  Recall_0  \\\n",
       "0   0.799270  0.859107     0.723404     0.838889  0.701031  0.853107   \n",
       "1   0.821168  0.875648     0.792683     0.833333  0.670103  0.903955   \n",
       "2   0.784672  0.862659     0.706522     0.824176  0.670103  0.847458   \n",
       "3   0.821168  0.874774     0.800000     0.829897  0.659794  0.909605   \n",
       "4   0.824818  0.880075     0.795181     0.837696  0.680412  0.903955   \n",
       "5   0.839416  0.906867     0.791209     0.863388  0.742268  0.892655   \n",
       "6   0.788321  0.860388     0.729412     0.814815  0.639175  0.870056   \n",
       "7   0.784672  0.846904     0.686275     0.843023  0.721649  0.819209   \n",
       "8   0.835766  0.909352     0.752475     0.884393  0.791667  0.859551   \n",
       "9   0.802198  0.861288     0.710000     0.855491  0.739583  0.836158   \n",
       "10  0.810147  0.873706     0.748716     0.842510  0.701579  0.869571   \n",
       "11  0.019817  0.019496     0.040907     0.019343  0.044372  0.030022   \n",
       "\n",
       "     F1score     Kappa       MCC  \n",
       "0   0.712042  0.558039  0.558201  \n",
       "1   0.726257  0.594846  0.599474  \n",
       "2   0.687831  0.523661  0.524088  \n",
       "3   0.723164  0.592880  0.598884  \n",
       "4   0.733333  0.604070  0.608139  \n",
       "5   0.765957  0.643925  0.644685  \n",
       "6   0.681319  0.523878  0.526438  \n",
       "7   0.703518  0.534631  0.535047  \n",
       "8   0.771574  0.643497  0.644003  \n",
       "9   0.724490  0.570304  0.570593  \n",
       "10  0.722948  0.578973  0.580955  \n",
       "11  0.027925  0.042464  0.042870  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_metrics.loc[11,:] = [np.std(np.array(accuracies)), np.std(np.array(aucs)), np.std(np.array(precisions_1)),\n",
    "                               np.std(np.array(precisions_0)), np.std(np.array(recalls_1)), np.std(np.array(recalls_0)),\n",
    "                            np.std(np.array(f1scores)), np.std(np.array(kappas)), np.std(np.array(MCCs))]\n",
    "\n",
    "fold_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7850f367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Precision_1</th>\n",
       "      <th>Precision_0</th>\n",
       "      <th>Recall_1</th>\n",
       "      <th>Recall_0</th>\n",
       "      <th>F1score</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fold_0</th>\n",
       "      <td>0.799270</td>\n",
       "      <td>0.859107</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>0.838889</td>\n",
       "      <td>0.701031</td>\n",
       "      <td>0.853107</td>\n",
       "      <td>0.712042</td>\n",
       "      <td>0.558039</td>\n",
       "      <td>0.558201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold_1</th>\n",
       "      <td>0.821168</td>\n",
       "      <td>0.875648</td>\n",
       "      <td>0.792683</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.670103</td>\n",
       "      <td>0.903955</td>\n",
       "      <td>0.726257</td>\n",
       "      <td>0.594846</td>\n",
       "      <td>0.599474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold_2</th>\n",
       "      <td>0.784672</td>\n",
       "      <td>0.862659</td>\n",
       "      <td>0.706522</td>\n",
       "      <td>0.824176</td>\n",
       "      <td>0.670103</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.687831</td>\n",
       "      <td>0.523661</td>\n",
       "      <td>0.524088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold_3</th>\n",
       "      <td>0.821168</td>\n",
       "      <td>0.874774</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.829897</td>\n",
       "      <td>0.659794</td>\n",
       "      <td>0.909605</td>\n",
       "      <td>0.723164</td>\n",
       "      <td>0.592880</td>\n",
       "      <td>0.598884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold_4</th>\n",
       "      <td>0.824818</td>\n",
       "      <td>0.880075</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.837696</td>\n",
       "      <td>0.680412</td>\n",
       "      <td>0.903955</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.604070</td>\n",
       "      <td>0.608139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold_5</th>\n",
       "      <td>0.839416</td>\n",
       "      <td>0.906867</td>\n",
       "      <td>0.791209</td>\n",
       "      <td>0.863388</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>0.892655</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.643925</td>\n",
       "      <td>0.644685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold_6</th>\n",
       "      <td>0.788321</td>\n",
       "      <td>0.860388</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.639175</td>\n",
       "      <td>0.870056</td>\n",
       "      <td>0.681319</td>\n",
       "      <td>0.523878</td>\n",
       "      <td>0.526438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold_7</th>\n",
       "      <td>0.784672</td>\n",
       "      <td>0.846904</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.843023</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>0.819209</td>\n",
       "      <td>0.703518</td>\n",
       "      <td>0.534631</td>\n",
       "      <td>0.535047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold_8</th>\n",
       "      <td>0.835766</td>\n",
       "      <td>0.909352</td>\n",
       "      <td>0.752475</td>\n",
       "      <td>0.884393</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.859551</td>\n",
       "      <td>0.771574</td>\n",
       "      <td>0.643497</td>\n",
       "      <td>0.644003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold_9</th>\n",
       "      <td>0.802198</td>\n",
       "      <td>0.861288</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.855491</td>\n",
       "      <td>0.739583</td>\n",
       "      <td>0.836158</td>\n",
       "      <td>0.724490</td>\n",
       "      <td>0.570304</td>\n",
       "      <td>0.570593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.810147</td>\n",
       "      <td>0.873706</td>\n",
       "      <td>0.748716</td>\n",
       "      <td>0.842510</td>\n",
       "      <td>0.701579</td>\n",
       "      <td>0.869571</td>\n",
       "      <td>0.722948</td>\n",
       "      <td>0.578973</td>\n",
       "      <td>0.580955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Std</th>\n",
       "      <td>0.019817</td>\n",
       "      <td>0.019496</td>\n",
       "      <td>0.040907</td>\n",
       "      <td>0.019343</td>\n",
       "      <td>0.044372</td>\n",
       "      <td>0.030022</td>\n",
       "      <td>0.027925</td>\n",
       "      <td>0.042464</td>\n",
       "      <td>0.042870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy       AUC  Precision_1  Precision_0  Recall_1  Recall_0  \\\n",
       "Fold_0  0.799270  0.859107     0.723404     0.838889  0.701031  0.853107   \n",
       "Fold_1  0.821168  0.875648     0.792683     0.833333  0.670103  0.903955   \n",
       "Fold_2  0.784672  0.862659     0.706522     0.824176  0.670103  0.847458   \n",
       "Fold_3  0.821168  0.874774     0.800000     0.829897  0.659794  0.909605   \n",
       "Fold_4  0.824818  0.880075     0.795181     0.837696  0.680412  0.903955   \n",
       "Fold_5  0.839416  0.906867     0.791209     0.863388  0.742268  0.892655   \n",
       "Fold_6  0.788321  0.860388     0.729412     0.814815  0.639175  0.870056   \n",
       "Fold_7  0.784672  0.846904     0.686275     0.843023  0.721649  0.819209   \n",
       "Fold_8  0.835766  0.909352     0.752475     0.884393  0.791667  0.859551   \n",
       "Fold_9  0.802198  0.861288     0.710000     0.855491  0.739583  0.836158   \n",
       "Mean    0.810147  0.873706     0.748716     0.842510  0.701579  0.869571   \n",
       "Std     0.019817  0.019496     0.040907     0.019343  0.044372  0.030022   \n",
       "\n",
       "         F1score     Kappa       MCC  \n",
       "Fold_0  0.712042  0.558039  0.558201  \n",
       "Fold_1  0.726257  0.594846  0.599474  \n",
       "Fold_2  0.687831  0.523661  0.524088  \n",
       "Fold_3  0.723164  0.592880  0.598884  \n",
       "Fold_4  0.733333  0.604070  0.608139  \n",
       "Fold_5  0.765957  0.643925  0.644685  \n",
       "Fold_6  0.681319  0.523878  0.526438  \n",
       "Fold_7  0.703518  0.534631  0.535047  \n",
       "Fold_8  0.771574  0.643497  0.644003  \n",
       "Fold_9  0.724490  0.570304  0.570593  \n",
       "Mean    0.722948  0.578973  0.580955  \n",
       "Std     0.027925  0.042464  0.042870  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_metrics.index = ['Fold_0','Fold_1','Fold_2','Fold_3','Fold_4','Fold_5','Fold_6','Fold_7','Fold_8','Fold_9','Mean','Std']\n",
    "fold_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fded0aaf",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f4630fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,:-2]\n",
    "y = data.iloc[:,-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3106d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\MY_FILES\\Anaconda\\envs\\thesis\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fd3ca86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LR_Final.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, 'LR_Final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a06557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
